# README
# Phillip Long
# January 26, 2024

# Convert NPY files generated by evaluate.py into WAV/MIDI audio files.

# IMPORTS
##################################################

import argparse
import logging
import sys
import shutil
from os.path import exists, basename, dirname
from os import makedirs
from glob import glob

import multiprocessing
from typing import List, Tuple

import numpy as np
import pandas as pd
from tqdm import tqdm

# add current directory to path for imports
from os.path import dirname, realpath
sys.path.insert(0, dirname(realpath(__file__)))

import decode
import encode
import representation

##################################################

# CONSTANTS
##################################################

CONDITIONAL_TYPES = ["total", "note", "expressive"]
GENERATION_TYPES = ["total", "note", "expressive"] 
EVAL_TYPES = ["joint"] + [f"conditional_{conditional_type}_{generation_type}" 
                         for conditional_type in CONDITIONAL_TYPES 
                         for generation_type in GENERATION_TYPES]

##################################################


# HELPER FUNCTIONS
##################################################

def determine_model_config(model_name: str) -> Tuple[str, str]:
    """
    Determine the evaluation subdirectory and optimal file format for a model.
    
    Parameters
    ----------
    model_name : str
        Name of the model to analyze.
        
    Returns
    -------
    Tuple[str, str]
        Tuple containing (eval_subdir, output_format).
        - eval_subdir: subdirectory within eval/ for this model
        - output_format: "WAV" for audio, "MIDI" for MIDI files
    """
    
    # check model type - must check econditional before conditional since it contains 'conditional'
    if "econditional" in model_name:
        return "conditional_note_total", "MIDI"
    elif "conditional" in model_name:
        return "conditional_expressive_total", "WAV"
    else:
        return "joint", "WAV"

##################################################


# PROCESS FILES AND MODELS
##################################################

def process_single_npy(
    npy_filepath: str,
    output_base_dir: str,
    encoding: dict,
    skip_existing: bool,
    ) -> Tuple[str, bool]:
    """
    Process a single NPY file into multiple output formats.
    
    Parameters
    ----------
    npy_filepath : str
        Path to the input NPY file.
    output_base_dir : str
        Base directory where outputs should be saved.
    encoding : dict
        Encoding configuration for decoding.
    skip_existing : bool
        Whether to skip processing if output files already exist.
        
    Returns
    -------
    Tuple[str, bool]
        Tuple containing (output_base_dir, success).
    """
    
    try:

        # create subdirectory based on NPY filename
        stem = basename(npy_filepath).replace(".npy", "")
        output_subdir = f"{output_base_dir}/{stem}"
        
        # define output file paths
        wav_filepath = f"{output_subdir}/output.wav"
        midi_filepath = f"{output_subdir}/output.mid"
        csv_filepath = f"{output_subdir}/expression.csv"
        
        # check if all output files exist and skip if we're not resetting
        if skip_existing and all(exists(fp) for fp in [wav_filepath, midi_filepath, csv_filepath]):
            logging.info(f"Skipping existing outputs for: {stem}")
            return output_subdir, True
        
        # create output subdirectory
        makedirs(output_subdir, exist_ok = True)
        
        # load NPY file and decode to music objects
        data = np.load(npy_filepath)
        music = decode.decode(
            codes = data, 
            encoding = encoding, 
            infer_metrical_time = True,
        )
        
        # write audio and MIDI files
        music.write(wav_filepath, kind = "audio")
        music.write(midi_filepath, kind = "midi")
        
        # extract expression text data for CSV
        expression_data = encode.extract_data(
            music = music, 
            use_implied_duration = False, 
            include_annotation_class_name = True
        )
        
        # filter to expression text features only
        expression_features = expression_data[expression_data[:, 0] == representation.EXPRESSIVE_FEATURE_TYPE_STRING]
        
        # extract relevant columns and convert to DataFrame
        expression_df_data = []
        time_dim = encoding["dimensions"].index("time") if "time" in encoding["dimensions"] else encoding["dimensions"].index("beat")
        value_dim = encoding["dimensions"].index("value")
        resolution = music.resolution
        for feature_row in expression_features:
            time_step = feature_row[time_dim]
            marking = encoding["code_value_map"][feature_row[value_dim]]
            time_seconds = music.get_real_time(time = time_step) # convert time step to seconds
            beat = time_step // resolution
            position = time_step % resolution
            expression_df_data.append({
                "marking": marking,
                "time": time_seconds,
                "beat": beat,
                "position": position
            })
        
        # create and save CSV
        expression_df = pd.DataFrame(expression_df_data)
        expression_df.to_csv(csv_filepath, index = False)
        
        return output_subdir, True
        
    # handle errors
    except Exception as e:
        logging.error(f"Failed to process {npy_filepath}: {e}")
        return "", False

def process_model(
    model_name: str, 
    base_dir: str, 
    output_base_dir: str, 
    encoding: dict,
    jobs: int,
    skip_existing: bool = False,
    ) -> Tuple[str, int, int, List[str]]:
    """
    Process all NPY files for a single model.
    
    Parameters
    ----------
    model_name : str
        Name of the model to process.
    base_dir : str
        Base directory containing models.
    output_base_dir : str
        Base output directory.
    encoding : dict
        Encoding configuration.
    jobs : int
        Number of parallel processes.
    skip_existing : bool, optional
        Skip files that already exist, by default False.
        
    Returns
    -------
    Tuple[str, int, int, List[str]]
        Tuple containing (model_name, total_files, success_files, output_filepaths).
    """
    
    # determine model configuration and setup output paths
    logging.info(f"Processing model: {model_name}")
    eval_subdir, output_format = determine_model_config(model_name)
    logging.info(f"  Using eval subdir: {eval_subdir}, output format: {output_format}")
    model_output_dir = f"{output_base_dir}/{model_name}"
    
    # find NPY files in the appropriate eval subdirectory
    eval_data_dir = f"{base_dir}/models/{model_name}/eval/{eval_subdir}/data"
    npy_files = glob(f"{eval_data_dir}/*.npy")
    if not npy_files:
        logging.warning(f"No NPY files found in {eval_data_dir}")
        return model_name, 0, 0, []
    logging.info(f"  Found {len(npy_files)} NPY files")
    
    # prepare arguments for multiprocessing
    process_args = []
    for npy_file in npy_files:
        process_args.append((npy_file, model_output_dir, encoding, skip_existing))
    
    # process files in parallel and collect results
    success_count = 0
    output_filepaths = []
    with multiprocessing.Pool(processes = jobs) as pool:
        results = pool.starmap(func = process_single_npy, iterable = process_args, chunksize = 1)
    for output_subdir, success in results:
        output_filepaths.append(output_subdir)
        if success:
            success_count += 1
    logging.info(f"  Completed: {success_count}/{len(npy_files)} files processed successfully")
    
    return model_name, len(npy_files), success_count, output_filepaths

##################################################


# PARSE ARGUMENTS
##################################################

def parse_args(args = None, namespace = None):
    """
    Parse command-line arguments.

    Parameters
    ----------
    args : list, optional
        List of argument strings to parse, by default None (uses sys.argv)
    namespace : argparse.Namespace, optional
        Namespace object to populate with parsed arguments, by default None
    
    Returns
    -------
    argparse.Namespace
        Parsed command-line arguments.
    """
    parser = argparse.ArgumentParser(description = "Convert NPY files to audio files")
    parser.add_argument("-b", "--base_dir", default = "/deepfreeze/pnlong/muspy_express/experiments/metrical", type = str, help = "Base directory containing models subdirectory")
    parser.add_argument("-o", "--output_dir", default = "/deepfreeze/pnlong/muspy_express/demo_samples", type = str, help = "Output directory for generated audio files")
    parser.add_argument("-j", "--jobs", default = 4, type = int, help = "Number of parallel processes")
    parser.add_argument("-v", "--verbose", action = "store_true", help = "Enable verbose logging")
    parser.add_argument("--reset", action = "store_true", help = "Remove existing output directories and regenerate all files")
    return parser.parse_args(args = args, namespace = namespace)

##################################################


# MAIN METHOD
##################################################

if __name__ == "__main__":
    
    # parse command-line arguments and setup directories
    args = parse_args()
    
    # determine experiment type from base directory name
    experiment_type = basename(args.base_dir) # "metrical" or "real"
    output_base_dir = f"{args.output_dir}/{experiment_type}"
    
    # handle reset or incremental processing modes
    if args.reset:
        if exists(output_base_dir):
            logging.info(f"Removing existing output directory: {output_base_dir}")
            shutil.rmtree(output_base_dir)
        logging.info(f"Reset mode: regenerating all files")
    else:
        logging.info(f"Incremental mode: skipping existing files")
    
    # create output directory if it doesn't exist
    makedirs(output_base_dir, exist_ok = True)
    
    # setup logging configuration
    log_filepath = f"{output_base_dir}/npy_to_audio.log"
    handlers = [logging.FileHandler(filename = log_filepath, mode = "w")]
    if args.verbose:
        handlers.append(logging.StreamHandler(stream = sys.stdout))
    logging.basicConfig(
        level = logging.INFO if args.verbose else logging.WARNING,
        format = "%(asctime)s - %(levelname)s - %(message)s",
        handlers = handlers
    )
    logging.info(f"Starting NPY to audio conversion")
    logging.info(f"Base directory: {args.base_dir}")
    logging.info(f"Output directory: {output_base_dir}")
    logging.info(f"Jobs: {args.jobs}")
    logging.info(f"Reset mode: {args.reset}")
    
    # load encoding configuration from base directory
    encoding_filepath = f"{args.base_dir}/encoding.json"
    if exists(encoding_filepath):
        encoding = representation.load_encoding(filepath = encoding_filepath)
    else:
        encoding = representation.get_encoding()
        logging.warning(f"Encoding file not found at {encoding_filepath}, using default encoding")
    
    # load list of models to process
    models_filepath = f"{args.base_dir}/models/models.txt"
    if not exists(models_filepath):
        logging.error(f"Models file not found: {models_filepath}")
        exit(1)
    
    # read model names from file
    with open(models_filepath, "r") as f:
        models = [line.strip() for line in f.readlines() if line.strip()]
    logging.info(f"Found {len(models)} models to process")
    
    # process all models and collect statistics
    total_files = 0
    total_success = 0
    all_output_filepaths = []
    model_stats = []
    for model in tqdm(models, desc = "Processing models"):

        # determine model configuration for statistics reporting
        eval_subdir, output_format = determine_model_config(model)
        model_name, model_total_files, model_success_files, model_output_files = process_model(
            model_name = model,
            base_dir = args.base_dir,
            output_base_dir = output_base_dir,
            encoding = encoding,
            jobs = args.jobs,
            skip_existing = not args.reset,
        )
        
        # accumulate statistics for overall reporting
        total_files += model_total_files
        total_success += model_success_files
        all_output_filepaths.extend(model_output_files)
        model_stats.append((model_name, model_total_files, model_success_files, eval_subdir, output_format))
    
    # generate and display comprehensive statistics
    logging.info(f"\n{'='*60}")
    logging.info(f"CONVERSION STATISTICS")
    logging.info(f"{'='*60}")
    logging.info(f"Mode: {'Reset (regenerate all)' if args.reset else 'Incremental (skip existing)'}")
    logging.info(f"Total models processed: {len(models)}")
    logging.info(f"Total NPY files processed: {total_files}")
    logging.info(f"Successfully processed: {total_success}")
    logging.info(f"Success rate: {(total_success/total_files*100):.1f}%" if total_files > 0 else "N/A")
    
    # display detailed model-specific statistics
    logging.info(f"\nModel-specific results:")
    for model_name, total, success, eval_subdir, output_format in model_stats:
        logging.info(f"  {model_name}:")
        logging.info(f"    Eval subdir: {eval_subdir}")
        logging.info(f"    Sample directories: {success}/{total} ({success/total*100:.1f}%)" if total > 0 else "0/0")
    
    # list all generated directories with status indicators
    logging.info(f"\nOutput directory: {output_base_dir}")
    logging.info(f"Sample directories created:")
    existing_count = 0
    for dirpath in sorted(all_output_filepaths):
        if exists(dirpath):
            logging.info(f"  ✓ {dirpath}")
            existing_count += 1
        else:
            logging.info(f"  ✗ {dirpath} (missing)")
    
    # report skipped directories count for incremental mode
    if not args.reset:
        logging.info(f"\nNote: {existing_count} sample directories already existed and were skipped during incremental processing")
    logging.info(f"\nConversion completed!")

##################################################
